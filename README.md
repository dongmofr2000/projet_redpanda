üöÄ Projet : Pipeline de Streaming PySpark & Redpanda (Kafka)
Ce document sert de documentation pour la preuve de concept (POC) d'un pipeline d'analyse de donn√©es de support client en temps r√©el.
Il montre l'ingestion de donn√©es de tickets via Redpanda (Kafka), leur traitement par PySpark Streaming avec agr√©gation en fen√™tre glissante, 
et l'exportation vers des fichiers Parquet pour le stockage analytique.


üèóÔ∏è 1. Architecture du Pipeline (Diagramme Mermaid)
Le diagramme de flux ci-dessous illustre l'architecture de votre pipeline de donn√©es, de la source (Producteur Python) au stockage final (Parquet).


<img width="1288" height="604" alt="image" src="https://github.com/user-attachments/assets/5f45bea3-191a-482f-b8da-d2ec20edd563" />
graph LR
    subgraph Producteur
        P[Producteur Python]
    end

    subgraph Broker_Kafka
        K(Redpanda / Kafka)
    end

    subgraph Processeur_Spark
        S[PySpark Streaming]
    end

    subgraph Destination
        D[Fichiers Parquet (Agr√©gations)]
    end

    P -->|Envoie Tickets JSON| K
    K -->|Lit en Temps R√©el| S
    S -->|Fen√™trage 1 min / Watermark| S
    S -->|√âcrit R√©sultats| D

Description du Flux :
1.	Producteur (P) : G√©n√®re des tickets de support fictifs au format JSON.
2.	Broker (K) : Redpanda (compatible Kafka) ing√®re et met en file d'attente les messages sur le topic support_tickets.
3.	Processeur (S) : PySpark lit le flux, applique une fen√™tre glissante d'une minute sur l'horodatage (timestamp) pour compter les tickets par priorit√©,
4.	et utilise un watermark d'une minute pour g√©rer les donn√©es en retard.
5.	Destination (D) : Les r√©sultats agr√©g√©s sont √©crits sur le syst√®me de fichiers dans des fichiers Parquet.
üõ†Ô∏è 2. Composants et Ex√©cution du POC
Ce projet est enti√®rement conteneuris√©. Les instructions ci-dessous supposent que vous √™tes dans le r√©pertoire racine du projet et que Docker Compose est install√©.
A. D√©marrage de l'Environnement
Le fichier docker-compose.yml construit les images Producteur et Processeur et d√©marre le broker Redpanda.
# 1. Construire les images et d√©marrer tous les services en arri√®re-plan
docker-compose up --build -d

B. Lancement du Producteur (Source)
Le producteur commence √† envoyer des tickets au topic Kafka (support_tickets).
# 2. Ex√©cuter le script du producteur dans son conteneur
docker exec -it ticket-producer python ticket_producer.py

C. Lancement du Processeur PySpark (ETL)
Le processeur Spark lit le flux, effectue l'agr√©gation, et √©crit en mode append dans le r√©pertoire d'export.
# 3. Ex√©cuter le script PySpark (en s'assurant d'inclure les packages Kafka)
docker exec -it spark-processor /spark/bin/spark-submit \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 \
    /opt/app/final_processor.py

D. Validation des R√©sultats
Apr√®s environ 90 √† 120 secondes (pour que Spark cl√¥ture la premi√®re fen√™tre de 1 minute), vous pouvez v√©rifier les fichiers Parquet :
# 4. Entrer dans le conteneur du processeur
docker exec -it spark-processor bash

# 5. Lister les fichiers Parquet export√©s
ls -R /opt/app/output/ticket_counts

Vous devriez voir des fichiers nomm√©s part-0000x-***.parquet.


üé• 3. Vid√©o de D√©monstration
Cette courte vid√©o explique le fonctionnement du pipeline, depuis le lancement du producteur jusqu'√† la v√©rification des fichiers Parquet g√©n√©r√©s par Spark.
Support Vid√©o : Loom
Description	Lien d'Int√©gration
D√©monstration du Pipeline ETL	[[Lien vers la Vid√©o Loom de D√©monstration]](https://www.loom.com/share/eb1422faadf04df5b8ca10dd998e974d?sid=055f35e1-9314-47e9-b45c-2915a90c930e)



